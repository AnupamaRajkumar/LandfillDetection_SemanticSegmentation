# -*- coding: utf-8 -*-
"""LandfillPipeline_VGG_FCN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FNmrYVe_Sfq1NurbCaNfJRMNam_Ve4kK

Solaris installation with gdal\
!add-apt-repository ppa:ubuntugis/ppa -y\
!apt-get update\
!apt-get install python-numpy gdal-bin libgdal-dev python3-rtree\

!pip install solaris==0.2.0
"""

!pip3 install torch torchvision
!pip3 install py7zr
!pip3 install rasterio
!pip3 install --upgrade earthpy

"""Solaris installation"""

!add-apt-repository ppa:ubuntugis/ppa -y
!apt-get update
!apt-get install python-numpy gdal-bin libgdal-dev python3-rtree\

!pip install solaris==0.2.0

#python imports
import py7zr
import os
import time
import numpy as np
import pandas as pd
from PIL import Image
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
import matplotlib.pyplot as plt

#earth observation python lib imports
import rasterio
import solaris as sol
import geopandas as gpd
import earthpy as et
import earthpy.spatial as es
import earthpy.plot as ep

#pytorch imports
import torch
from torch import nn
from torchvision import datasets, transforms, models
import torchvision.transforms.functional as TF
import torch.optim as optim
from torch.optim import lr_scheduler
from torchvision.models.vgg import VGG

device = torch.device('cuda:0' if torch.cuda.is_available() else "cpu")
print('Using device:', device)

"""load dataset from github\
repo path : https://github.com/AnupamaRajkumar/LandfillDataset.git

"""

#!apt-get update && apt-get upgrade
#!apt-get install git-lfs

"""Use this if downloading from git (ie. LFS quota available!)"""

#!git lfs clone https://github.com/AnupamaRajkumar/LandfillDataset.git

"""Declaring paths"""

#All the paths related to multispectral dataset
Multispectral_path = './Multispectral.7z'                                       #path of zip files
TIF_file_path = './MultiSpectral/HR_TIF_Files.7z'
JSON_file_path = './MultiSpectral/LandfillCoordPolygons.7z'
json_path = './LandfillCoordPolygons'
train_path = './HR_TIF_Files'
train_labels = './MultiSpectral/MultiSpectralData.csv'

#All the paths related to pansharpened datset
Pansharpened_path = './Pansharpened.7z'
PanSharpened_TIF_file_path = './Pansharpened/HR_TIF_Files.7z'

#Output directory path
out_path = './'                                                                 #output path of extracted files

#dimensions of the figure to be displayed
width = 15
height = 15
num_images = 6

"""Extract the images from zip folder"""

def ExtractFiles(in_path, out_path):
  with py7zr.SevenZipFile(in_path, mode='r') as z:
    z.extractall(out_path)

"""Use following functions downloading dataset from dropbox

Multispectral dataset
"""

def DownloadMultiSpectralDataset():
  !wget https://www.dropbox.com/s/2r0b8goiytyh0fc/MultiSpectral.7z?dl=0
  os.rename('MultiSpectral.7z?dl=0', 'Multispectral.7z')
  ExtractFiles('Multispectral.7z', out_path)

"""Pansharpened dataset"""

def DownloadPanSharpenedDataset():
  !wget https://www.dropbox.com/s/qb0dfxjafuodc27/Pansharpened.7z?dl=0
  os.rename('Pansharpened.7z?dl=0', 'Pansharpened.7z')
  ExtractFiles('Pansharpened.7z', out_path)
  #os.rmdir(Pansharpened_path)

DownloadMultiSpectralDataset()

"""Extracting the multispectral dataset\
Run this cell if working with multispectral dataset
"""

#extract multispectral images
ExtractFiles(TIF_file_path , out_path)
#extract multispectral json files
ExtractFiles(JSON_file_path , out_path)

"""Extracting the pansharpened dataset\
Run this cell if working with pansharpened dataset

" 
  #extract pansharpened images
  ExtractFiles(PanSharpened_TIF_file_path , out_path)
  #extract multispectral json files
  #ExtractFiles(JSON_file_path , out_path)
"""

def im_convert(image_name, channels):
  image = rasterio.open(os.path.join(train_path, image_name)).read()
  #false color composite visualisation
  if(channels == 8):
    raster = np.dstack((image[4,:,:], image[2,:,:],image[1,:,:])) 
  elif(channels == 4):
    raster = np.dstack((image[2,:,:], image[1,:,:],image[0,:,:]))
  return raster

"""Displaying a few images from the folder"""

train_files = os.listdir(train_path)

num_of_train_images = len([name for name in train_files if os.path.isfile(os.path.join(train_path, name))])
print("Number of images images:", num_of_train_images)
selected = np.random.choice(num_of_train_images, num_images)

fig = plt.figure(figsize = (25,25))
print('-------------multispectral images----------------')

for i, ind in enumerate(selected):
  raster_arr = rasterio.open(os.path.join(train_path, train_files[ind])).read()
  channels = raster_arr.shape[0]
  print("image : " , train_files[ind], "channels :", channels)
  #false color composite visualisation
  if(channels == 8):
    rgb = (4,2,1)             #R, G, B bands
  elif(channels == 4):
    rgb = (2,1,0)             #R, G, B bands

  ep.plot_rgb(raster_arr, rgb=rgb, stretch=True, title=train_files[ind])
  plt.show()
  #plt.imshow(raster_fcc)
  #plt.title(train_files[ind])

"""Loading the dataset from the split dataset

referred from:\
https://github.com/tkshnkmr/frcnn_medium_sample,
https://pytorch.org/tutorials/beginner/data_loading_tutorial.html

Checking the proportion of data classes in the dataset
"""

dataFrame = pd.read_csv(train_labels, usecols=["Idx", "Image Index", "IsLandfill"])
data = dataFrame.values.tolist()
label = pd.read_csv(train_labels, usecols=["IsLandfill"])


def get_class_count(data):
  grp = data.groupby(["IsLandfill"]).nunique()
  return{key : grp[key] for key in list(grp.keys())}

def get_class_proportion(data):
  class_count = get_class_count(data)
  return {val[0]: round(val[1]/data.shape[0], 4) for val in class_count.items()}

#print("Dataset class counts:", get_class_count(dataFrame))
print("Dataset class proportions:", get_class_proportion(dataFrame))

"""Tackling the imabalanced dataset
1. Undersampling : Randomly delete some observations from the majority class in order to match the numbers with the minority class. Suitable when there is a lot of data
2. Oversampling : Add copies of instances from under-represented class. Suitable for smaller dataset

We will use random over-sampling as we have a small set of data - TBD
"""



"""Stratified and random split into train, test and validation datasets"""

train_ratio = 0.75
test_ratio = 0.25
#validation_ratio = 0.10

train_idx, test_idx, train_lab, test_lab = train_test_split(dataFrame, label, 
                                                            test_size = test_ratio, 
                                                            shuffle=True, stratify=label)

"""
train_idx, val_idx, train_lab, val_lab = train_test_split(train_idx, train_lab,
                                                          test_size = validation_ratio,
                                                          shuffle=True, stratify=train_lab)
"""


print("Train size is {}, test size is {}".format(len(train_idx), len(test_idx)))

print("****************Dataset proportion after stratified splitting***************")
print("Train data class proportions:", get_class_proportion(train_idx))
print("Test data class proportions:", get_class_proportion(test_idx))

#parameters
batch_size = 2
num_workers = 1
num_classes = 2       #landfill or background
epochs = 100
lr = 1e-4
w_decay = 1e-5
momentum = 0.9
step_size = 50
gamma = 0.5
image_size = 512
patch_width = 512
patch_height= 512

"""Custom augmentations using imgaug library

Add transform to resize and normalise the images
"""

#transforms.ToPILImage(),
#transforms.Resize((224, 224)),
#transforms.ToTensor(),

train_transforms = transforms.Compose([transforms.ToTensor(),
                                       transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])

test_transforms = transforms.Compose([transforms.ToTensor(),
                                      transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])

class CustomLandfillDataset(torch.utils.data.Dataset):
  def __init__(self, data, transforms, num_classes=num_classes):                             #transforms
    self.data = data
    self.transforms = transforms
    self.json_frame = pd.read_csv(train_labels, usecols=["json index"])
    self.isLandfillList = pd.read_csv(train_labels, usecols=["IsLandfill"]).values.tolist()
    self.num_classes = num_classes

  def __len__(self):
    return len(self.data)

  def __getitem__(self, index):
    #image id
    img_id = self.data[index][0]
    #path for image
    img_name = self.data[index][1]
    #read the image to extract number of channels
    raster_img = rasterio.open(os.path.join(train_path, img_name)).read()
    #change datatype of image from uint16 to int32
    raster_img = raster_img.astype('float32')
    #print(raster_img.shape)
    raster_channels = raster_img.shape[0]
    #create another image variable
    image = raster_img

    #in case the image is smaller than 512x512
    if((raster_img.shape[1] < patch_height) or (raster_img.shape[2] < patch_width)):
      image = np.zeros((patch_height, patch_width), dtype=np.float32)
      image[0:raster_img.shape[1], 0:raster_img.shape[2]] = raster_img[0,:,:]
      image = np.expand_dims(image, axis=0)
      for c in range(raster_channels-1):
        resized_raster = np.zeros((patch_height, patch_width), dtype=np.float32)
        #np.copy(resized_raster, raster_img[c,:,:])
        resized_raster[0:raster_img.shape[1], 0:raster_img.shape[2]] = raster_img[c+1,:,:]
        resized_raster = np.expand_dims(resized_raster,axis=0)
        image = np.vstack((image, resized_raster)) 

    #stack dummy channels if number of channels less than 8
    if(raster_channels < 8):
      diff = 8 - raster_channels
      dummy_channels = np.zeros((patch_height, patch_width), dtype=np.float32)
      dummy_channels = np.expand_dims(dummy_channels, axis=0)
      #print(dummy_channels.shape)
      for i in range(diff-1):
        dummy_channel = np.zeros((patch_height, patch_width), dtype=np.float32)
        dummy_channel = np.expand_dims(dummy_channel, axis=0)
        dummy_channels = np.vstack((dummy_channel, dummy_channels))
        #print("dummy_channel=", dummy_channel.shape,"dummy_channels=", dummy_channels.shape)
      image = np.vstack((image, dummy_channels))
      #print("image", image.shape)

    rgb_image = image.transpose(1,2,0)         
    if(raster_channels == 8):
      rgb_image = np.dstack((image[4,:,:], image[2,:,:], image[1,:,:]))
    elif(raster_channels == 4):
      rgb_image = np.dstack((image[2,:,:], image[1,:,:], image[0,:,:]))
    min = np.min(rgb_image)
    max = np.max(rgb_image)
    #before normalisation, the image should be brought into a 0 - 1 range (standardisation)
    rgb_image = (rgb_image - min) / (max - min)
    if self.transforms is not None:
      rgb_image = self.transforms(rgb_image)

    #mask
    json_list = self.json_frame.values.tolist()
    json_name = json_list[img_id-1][0]
    IsLandfill = self.isLandfillList[img_id-1][0]
    if(IsLandfill):
      fp_mask = sol.vector.mask.footprint_mask(df=os.path.join(json_path, json_name), 
                                               reference_im=os.path.join(train_path, img_name))
      fp_mask = fp_mask.astype('float32')
    else:
      fp_mask = np.zeros((patch_height, patch_width), dtype=np.float32)
    
    #create another mask variable
    mask = fp_mask
    #in case the mask size is smaller than 512x512
    if((fp_mask.shape[0] < patch_height) or (fp_mask.shape[1] < patch_width)):
      mask = np.zeros((patch_height, patch_width), dtype=np.float32)
      mask[0:fp_mask.shape[0], 0:fp_mask.shape[1]] = fp_mask
    #one hot encoding of the mask depending on the number of classes
    mask_hotEnc = torch.zeros(self.num_classes, patch_height, patch_width)
    for n in range(self.num_classes):
      mask_hotEnc[n][mask==n] = 1
      
    #mask = np.expand_dims(mask, axis=0)
    #print("image shape:",image.shape," mask shape:", mask.shape)
    #print("transformed image shape:", rgb_image.shape, " transformed mask shape:", mask.shape)

    img_info = {}
    img_info["RGBimage"] = rgb_image
    img_info["image"] = image
    img_info["mask"] = mask
    img_info["maskHotEnc"] = mask_hotEnc
    img_info["channels"] = raster_channels
    img_info["name"] = img_name
    img_info["image_id"] = img_id

    return img_info

"""Create respective train and test datasets and create a dataloader"""

#train_idx and test_idx are dataframes
train_dataset = CustomLandfillDataset(data=train_idx.values.tolist(), transforms=train_transforms)                               
test_dataset = CustomLandfillDataset(data=test_idx.values.tolist(), transforms=test_transforms)                                

#data loader
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                           batch_size=batch_size,
                                           num_workers=num_workers,
                                           shuffle=True)

test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                           batch_size=batch_size,
                                           num_workers=num_workers,
                                           shuffle=True)

print("length of training dataset:", len(train_dataset))
print("length of test dataset:", len(test_dataset))
print("length of training loader:", len(train_loader))
print("length of test loader:", len(test_loader))

"""Checking out some images and their corresponding masks"""

img_info = next(iter(train_loader))

image = img_info['image']
rgb_img = img_info['RGBimage']
mask = img_info['mask']
mask_hotEnc = img_info['maskHotEnc']
channels = img_info['channels']

print("image shape:", image.shape, "image mask:", mask.shape, "channels:", channels)
print("transformed image shape:", rgb_img.shape, "hot encoded mask:", mask_hotEnc.shape)
#check the min and max values of the image and mask to check if it is standardised and normalised
rgbArr = rgb_img[0].detach().numpy()
maskArr = mask_hotEnc[0].detach().numpy()
print("Min value of image:", np.min(rgbArr), " Max value of image:", np.max(rgbArr))
print("Min value of mask:", np.min(maskArr), " Max value of mask:", np.max(maskArr))

test_info = next(iter(test_loader))

image = img_info['image']
rgb_img = img_info['RGBimage']
mask = img_info['mask']
mask_hotEnc = img_info['maskHotEnc']
channels = img_info['channels']

print("image shape:", image.shape, "image mask:", mask.shape, "channels:", channels)
print("transformed image shape:", rgb_img.shape, "hot encoded mask:", mask_hotEnc.shape)
#check the min and max values of the image and mask to check if it is standardised and normalised
rgbArr = rgb_img[0].detach().numpy()
maskArr = mask_hotEnc[0].detach().numpy()
print("Min value of image:", np.min(rgbArr), " Max value of image:", np.max(rgbArr))
print("Min value of mask:", np.min(maskArr), " Max value of mask:", np.max(maskArr))

"""Display the images from the train set"""

fig = plt.figure(figsize=(width, height))

for idx in np.arange(batch_size):
  #plt.subplot(2, 3, idx+1)
  channels = img_info["channels"][idx].item() 
  name = img_info["name"][idx]
  image = img_info["image"][idx].numpy()
  #false color composite visualisation
  if(channels == 8):
    rgb = (4,2,1)             #R, G, B bands
  elif(channels == 4):
    rgb = (2,1,0)             #R, G, B bands
  ep.plot_rgb(image, rgb=rgb, stretch=True, title=name)
  plt.show()

fig = plt.figure(figsize=(width, height))
#the distorted images are due to normalisation
for idx in np.arange(batch_size):
  plt.subplot(2, 1, idx+1)
  channels = img_info["channels"][idx].item() 
  name = img_info["name"][idx]
  image = img_info["RGBimage"][idx].numpy()
  image = image.transpose(1,2,0)
  plt.imshow((image).astype(np.uint8))
  plt.title(name)

"""Draw polygon on image:

"""

json_frame = pd.read_csv(train_labels, usecols=["json index"])
json_list = json_frame.values.tolist()

for idx in np.arange(batch_size):
  id = img_info["image_id"][idx].item()
  json_name = json_list[id-1][0]
  print(json_name)
  gdf = gpd.read_file(os.path.join(json_path, json_name))

count = 0
fig, axs = plt.subplots(batch_size, 3, figsize = (20, 20))

for idx in np.arange(batch_size):
  id = img_info["image_id"][idx].item()
  #print(id)
  json_name = json_list[id-1][0]
  name = img_info["name"][idx]
  #print(name)
  channels = img_info["channels"][idx].item()  
  image = im_convert(name, channels)
  mask = img_info["mask"][idx].detach().numpy()
  #print(mask.shape)
  axs[count][0].title.set_text(name)
  axs[count][0].imshow(image)

  axs[count][1].title.set_text('Mask')
  axs[count][1].imshow(mask, cmap='gray')

  axs[count][2].title.set_text(name+" with mask")
  axs[count][2].imshow(image)
  axs[count][2].imshow(mask, cmap='ocean', alpha=0.4)
  count += 1

fig.tight_layout()

"""Case 1: Train with stratified dataset with no transforms

Training a model on the training batches

Freeze the weights until some intermediate layers to finetune the rest of the network\
https://discuss.pytorch.org/t/how-the-pytorch-freeze-network-in-some-layers-only-the-rest-of-the-training/7088/2  \
ref: https://github.com/pochih/FCN-pytorch/tree/8436fab3586f118eb36265dab4c5f900748bb02d

FCN encoder
"""

pretrained = True
enc_model = 'vgg13'

maxpool_ranges = {
    'vgg11': ((0, 3), (3, 6),  (6, 11),  (11, 16), (16, 21)),
    'vgg13': ((0, 5), (5, 10), (10, 15), (15, 20), (20, 25)),
    'vgg16': ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31)),
    'vgg19': ((0, 5), (5, 10), (10, 19), (19, 28), (28, 37))
}

def load_model(enc_model=enc_model):
  if enc_model == 'vgg11':
    model = models.vgg11(pretrained=pretrained)
  if enc_model == 'vgg13':
    model = models.vgg13(pretrained=pretrained)
  if enc_model == 'vgg16':
    model = models.vgg16(pretrained=pretrained)
  if enc_model == 'vgg19':
    model = models.vgg19(pretrained=pretrained)
  print(model)
  return model

class VGGNet():
  def __init__(self, pretrained=pretrained, enc_model=enc_model, requires_grad=True, remove_fc=True):
    self.model = load_model(enc_model)
    self.model = self.model.to(device)
    #super().__init__(load_model(enc_model))
    self.maxpool_ranges = maxpool_ranges[enc_model]
    #use this piece of code to freeze and unfreeze layers for training
    cnt = 0
    for child in self.model.children():
        for name, param in child.named_parameters():
          if cnt < 4:
            #print(name)
            param.requires_grad = False
          cnt += 1

    if requires_grad == False:
      for param in self.model.parameters():
        param.requires_grad = False

    if remove_fc:
      del self.model.avgpool
      del self.model.classifier
        
    print("Params to learn:")
    for name, param in self.model.named_parameters():
      if param.requires_grad == True:
        print("\t", name, "\t", param.size())

  def forward(self, x):
    output = {}
    # get the output of each maxpooling layer (5 maxpool in VGG net)
    for idx in range(len(self.maxpool_ranges)):
        for layer in range(self.maxpool_ranges[idx][0], self.maxpool_ranges[idx][1]):
          #print(layer)
          x = self.model.features[layer](x)
        output["x%d"%(idx+1)] = x
    #print(output)
    return output

VGGOut = VGGNet(enc_model=enc_model, pretrained=pretrained)

"""FCN decoder"""

class FCN8s(nn.Module):
  def __init__(self, pretrained_model=VGGOut, num_classes=num_classes):
    super().__init__()
    self.num_classes = num_classes
    self.pretrained_model = pretrained_model
    self.relu    = nn.ReLU(inplace=True)
    self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)
    self.bn1     = nn.BatchNorm2d(512)
    self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)
    self.bn2     = nn.BatchNorm2d(256)
    self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)
    self.bn3     = nn.BatchNorm2d(128)
    self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)
    self.bn4     = nn.BatchNorm2d(64)
    self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)
    self.bn5     = nn.BatchNorm2d(32)
    self.classifier = nn.Conv2d(32, num_classes, kernel_size=1)
    #output = self.pretrained_model.forward()
    #print(output)

  def forward(self, x):
    output = self.pretrained_model.forward(x)
    #print(output)
    x5 = output['x5']                                 # size=(N, 512, x.H/32, x.W/32)
    #print(x5)
    x4 = output['x4']                                 # size=(N, 512, x.H/16, x.W/16)
    #print(x4)
    x3 = output['x3']                                 # size=(N, 256, x.H/8,  x.W/8)
    #print(x3)

    score = self.relu(self.deconv1(x5))               # size=(N, 512, x.H/16, x.W/16)
    #print(score)
    score = self.bn1(score + x4)                      # element-wise add, size=(N, 512, x.H/16, x.W/16)
    score = self.relu(self.deconv2(score))            # size=(N, 256, x.H/8, x.W/8)
    score = self.bn2(score + x3)                      # element-wise add, size=(N, 256, x.H/8, x.W/8)
    score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)
    score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)
    score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)
    score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)

    return score  # size=(N, n_class, x.H/1, x.W/1)

FCN_model = FCN8s(pretrained_model=VGGOut, num_classes=num_classes)
FCN_model = FCN_model.to(device)
#out = FCN_model(rgb_img)

"""Loss and optimiser"""

criterion = nn.BCEWithLogitsLoss() #BCEWithLogits()
optimizer = optim.SGD(FCN_model.parameters(), lr=lr, momentum=momentum)     #, momentum=momentum
#scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)  # decay LR by a factor of 0.5 every 30 epochs

def modelPath(enc_model=enc_model):
  if(enc_model == 'vgg11'):
    model_path = './vgg_11_FCN.pth'
  if(enc_model == 'vgg13'):
    model_path = './vgg_13_FCN.pth'
  if(enc_model == 'vgg16'):
    model_path = './vgg_16_FCN.pth'
  if(enc_model == 'vgg19'):
    model_path = './vgg_19_FCN.pth'

  return model_path

#https://github.com/pochih/FCN-pytorch/blob/8436fab3586f118eb36265dab4c5f900748bb02d/python/train.py
model_path = modelPath(enc_model)
score_dir = './'
IU_scores    = np.zeros((epochs, num_classes))
pixel_scores = np.zeros(epochs)
train_losses = []
val_losses = []
ious_mean = []
ious_ = []
pixelAccs_mean = []
accuracy = []

def TrainFunction():
  for epoch in range(epochs):
    t0 = time.time()
    train_loss = 0.0
    training_accuracy = 0.0
    total_train = 0.0
    correct_train = 0.0
    #FCN_model.train()
    for batch, img_info in enumerate(train_loader):
      rgb_img = img_info['RGBimage']
      mask_hotEnc = img_info['maskHotEnc']
      rgb_img = rgb_img.to(device)
      mask_hotEnc = mask_hotEnc.to(device)
      outputs = FCN_model(rgb_img)
      loss = criterion(outputs, mask_hotEnc)
      train_loss += loss.item()*rgb_img.size(0)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

      #accuracy
      _, predicted = torch.max(outputs, 1)
      total_train += mask_hotEnc.nelement()
      for p, m in zip(predicted, mask_hotEnc):
        correct_train += (p == m).sum()
      
      #print("epoch: {}/{}, batch: {}".format(epoch, epochs, batch))
    training_accuracy = 100* (correct_train / total_train)
    train_loss = train_loss/len(train_loader)
    train_losses.append(train_loss)
    accuracy.append(training_accuracy.item())
    print("Finish epoch: {}/{}, Loss in epoch:{}, training accuracy:{}, time elapsed: {}".format(epoch, 
                                                                                                 epochs, 
                                                                                                 train_loss, 
                                                                                                 training_accuracy,
                                                                                                 time.time() - t0))
    torch.save(FCN_model.state_dict(), model_path)
    ValFunction(epoch)

def ValFunction(epoch):
  FCN_model.eval()
  total_ious = []
  pixel_accs = []
  for batch, img_info in enumerate(test_loader):
    val_loss = 0.0
    total_val = 0.0
    correct_val = 0.0
    val_accuracy = 0.0
    rgb_img = img_info['RGBimage']
    mask_hotEnc = img_info['maskHotEnc']
    target = img_info['mask']
    rgb_img = rgb_img.to(device)
    mask_hotEnc = mask_hotEnc.to(device)
    target = target.to(device)
    outputs = FCN_model(rgb_img)
    loss = criterion(outputs, mask_hotEnc)
    val_loss += loss.item()*rgb_img.size(0)
    #outputs = outputs.data.cpu().numpy()
    #batchSize, _, h, w = outputs.shape
    #predict = outputs.transpose(0, 2, 3, 1).reshape(-1, num_classes).argmax(axis=1).reshape(batchSize, h, w)  
    #print(predict.shape)
    #determine performance metrics

    _, predicted = torch.max(outputs, 1)
    total_val += mask_hotEnc.nelement()
    for p, t in zip(predicted, target):
      correct_val += (p == t).sum()

    for p, t in zip(predicted, target):
      total_ious.append(iou(p, t))
      #pixel_accs.append(pixel_accuracy(p, t))

  #calculating average IOUs
  #print(batch, predict.shape)
  total_ious = np.array(total_ious).T  # n_class * val_len
  #ious = np.nanmean(total_ious, axis=1)
  #print(type(ious))
  pixel_accs = np.array(pixel_accs).mean()
  val_accuracy = 100 * (correct_val / total_val)
  val_loss = val_loss/len(test_loader)
  print("epoch: {}/{}, validation loss: {}, validation accuracy: {}".format(epoch, 
                                                                            epochs, 
                                                                            val_loss,
                                                                            val_accuracy))
  #ious_.append(ious)
  #ious_mean.append(np.nanmean(ious))
  pixelAccs_mean.append(val_accuracy)
  #IU_scores[epoch] = ious
  val_losses.append(val_loss)
  #np.save(os.path.join(score_dir, "meanIU"), IU_scores)
  pixel_scores[epoch] = pixel_accs
  np.save(os.path.join(score_dir, "meanPixel"), pixel_scores)

"""Common semantic segmentation performance metrics"""

def iou(pred, target):
  ious = []
  for cls in range(num_classes):
    pred_inds = (pred == cls)
    target_inds = (target == cls)
    intersection = pred_inds[target_inds].sum()
    union = pred_inds.sum() + target_inds.sum() - intersection
    if(union == 0):
      ious.append(float('nan'))
    else:
      ious.append(float(intersection) / max(union, 1))
  #print(ious)
  return ious

"""Train the model and run validations"""

TrainFunction()

"""Plot the loss and scores"""

plt.plot(train_losses, label='Training loss')
plt.plot(val_losses, label = 'Validation loss')
plt.legend()

plt.plot(accuracy, label='Training pixel accuracy')
plt.plot(pixelAccs_mean, label='Validation pixel accuracy')
plt.legend()

plt.plot(ious_, label='IOU')
plt.plot(ious_mean, label = 'Mean IOUs')