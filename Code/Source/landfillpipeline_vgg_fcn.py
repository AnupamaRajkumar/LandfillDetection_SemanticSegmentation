# -*- coding: utf-8 -*-
"""LandfillPipeline_VGG_FCN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s14UNkOUsG95MjqtUVkf-9IDtavMhNEw

Solaris installation with gdal\
!add-apt-repository ppa:ubuntugis/ppa -y\
!apt-get update\
!apt-get install python-numpy gdal-bin libgdal-dev python3-rtree\

!pip install solaris==0.2.0
"""

!pip3 install torch torchvision
!pip3 install py7zr
!pip3 install rasterio
!pip3 install --upgrade earthpy

"""Solaris installation"""

!add-apt-repository ppa:ubuntugis/ppa -y
!apt-get update
!apt-get install python-numpy gdal-bin libgdal-dev python3-rtree\

!pip install solaris==0.2.0

!pip3 install --upgrade tensorboard
!pip3 install pytorch-lightning

#python imports
import py7zr
import os
import time
import random
import numpy as np
import pandas as pd
from PIL import Image
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.utils import resample
import matplotlib.pyplot as plt

#earth observation python lib imports
import rasterio
import solaris as sol
import geopandas as gpd
import earthpy as et
import earthpy.spatial as es
import earthpy.plot as ep

#pytorch imports
import torch
from torch import nn
from torchvision import datasets, transforms, models
import torchvision.transforms.functional as TF
import torch.optim as optim
from torch.optim import lr_scheduler
from pytorch_lightning.callbacks.early_stopping import EarlyStopping

device = torch.device('cuda:0' if torch.cuda.is_available() else "cpu")
print('Using device:', device)

"""load dataset from github\
repo path : https://github.com/AnupamaRajkumar/LandfillDataset.git

"""

#!apt-get update && apt-get upgrade
#!apt-get install git-lfs

"""Use this if downloading from git (ie. LFS quota available!)"""

#!git lfs clone https://github.com/AnupamaRajkumar/LandfillDataset.git

"""Declaring paths"""

#All the paths related to multispectral dataset
Multispectral_path = './Multispectral.7z'                                       #path of zip files
TIF_file_path = './MultiSpectral/HR_TIF_Files_MultiSpectral.7z'
JSON_file_path = './MultiSpectral/LandfillCoordPolygons.7z'
multispectral_json = './LandfillCoordPolygonMulti/LandfillCoordPolygons'
multispectral_path = './MultiSpectralTIF/HR_TIF_Files'
multispectral_labels = './MultiSpectral/MultiSpectralData.csv'

#All the paths related to pansharpened datset
Pansharpened_path = './Pansharpened.7z'
PanSharpened_TIF_file_path = './Pansharpened/HR_TIF_Files.7z'
PanSharpened_JSON_file_path = './Pansharpened/LandfillCoordPolygons.7z'
pansharpened_json = './LandfillCoordPolygons'
train_path = './HR_TIF_Files'
train_labels = './Pansharpened/PanSharpenedData.csv'
#Output directory path
out_path = './'                                                                 #output path of extracted files

#dimensions of the figure to be displayed
width = 15
height = 15
num_images = 6

"""Extract the images from zip folder"""

def ExtractFiles(in_path, out_path):
  with py7zr.SevenZipFile(in_path, mode='r') as z:
    z.extractall(out_path)

"""Use following functions downloading dataset from dropbox

Multispectral dataset
"""

def DownloadMultiSpectralDataset():
  !wget https://www.dropbox.com/s/s0lrmwbzkrwllgw/MultiSpectral.7z?dl=0
  os.rename('MultiSpectral.7z?dl=0', 'Multispectral.7z')
  ExtractFiles('Multispectral.7z', out_path)

"""Pansharpened dataset"""

def DownloadPanSharpenedDataset():
  !wget https://www.dropbox.com/s/y048lu2ehtsmo3r/Pansharpened.7z?dl=0
  os.rename('Pansharpened.7z?dl=0', 'Pansharpened.7z')
  ExtractFiles('Pansharpened.7z', out_path)
  #os.rmdir(Pansharpened_path)

def DownloadPreTrainedModels():
  !wget https://www.dropbox.com/s/aysve9jphkm5yva/PreTrainedModels.7z?dl=0
  os.rename('PreTrainedModels.7z?dl=0', 'PreTrainedModels.7z')
  ExtractFiles('PreTrainedModels.7z', out_path)

DownloadPanSharpenedDataset()
DownloadMultiSpectralDataset()

DownloadPreTrainedModels()

"""Extracting the multispectral dataset\
Run this cell if working with multispectral dataset

#extract multispectral images
ExtractFiles(TIF_file_path , out_path)
#extract multispectral json files
ExtractFiles(JSON_file_path , out_path)

Extracting the pansharpened dataset\
Run this cell if working with pansharpened dataset
"""

#extract pansharpened images
ExtractFiles(PanSharpened_TIF_file_path , out_path)
#extract multispectral json files
ExtractFiles(PanSharpened_JSON_file_path , out_path)

def im_convert(image_name, channels):
  image = rasterio.open(os.path.join(train_path, image_name)).read()
  #false color composite visualisation
  if(channels == 8):
    raster = np.dstack((image[4,:,:], image[2,:,:],image[1,:,:])) 
  elif(channels == 4):
    raster = np.dstack((image[2,:,:], image[1,:,:],image[0,:,:]))
  return raster

"""Displaying a few images from the folder"""

train_files = os.listdir(train_path)

num_of_train_images = len([name for name in train_files if os.path.isfile(os.path.join(train_path, name))])
print("Number of images images:", num_of_train_images)
selected = np.random.choice(num_of_train_images, num_images)

fig = plt.figure(figsize = (25,25))
print('-------------multispectral images----------------')

for i, ind in enumerate(selected):
  raster_arr = rasterio.open(os.path.join(train_path, train_files[ind])).read()
  channels = raster_arr.shape[0]
  print("image : " , train_files[ind], "channels :", channels)
  #false color composite visualisation
  if(channels == 8):
    rgb = (4,2,1)             #R, G, B bands
  elif(channels == 4):
    rgb = (2,1,0)             #R, G, B bands

  ep.plot_rgb(raster_arr, rgb=rgb, stretch=True, title=train_files[ind])
  plt.show()
  #plt.imshow(raster_fcc)
  #plt.title(train_files[ind])

"""Loading the dataset from the split dataset

referred from:\
https://github.com/tkshnkmr/frcnn_medium_sample,
https://pytorch.org/tutorials/beginner/data_loading_tutorial.html

Checking the proportion of data classes in the dataset
"""

dataFrame = pd.read_csv(train_labels, usecols=["Idx", "Image Index", "IsLandfill"])
data = dataFrame.values.tolist()
label = pd.read_csv(train_labels, usecols=["IsLandfill"])


def get_class_count(data):
  grp = data.groupby(["IsLandfill"]).nunique()
  return{key : grp[key] for key in list(grp.keys())}

def get_class_proportion(data):
  class_count = get_class_count(data)
  return {val[0]: round(val[1]/data.shape[0], 4) for val in class_count.items()}

#print("Dataset class counts:", get_class_count(dataFrame))
print("Dataset class proportions:", get_class_proportion(dataFrame))

"""Tackling the imabalanced dataset
1. Undersampling : Randomly delete some observations from the majority class in order to match the numbers with the minority class. Suitable when there is a lot of data
2. Oversampling : Add copies of instances from under-represented class. Suitable for smaller dataset

We will use random over-sampling as we have a small set of data - TBD
"""



"""Stratified and random split into train, test and validation datasets"""

train_ratio = 0.70
test_ratio = 0.20
#validation_ratio = 0.10

train_idx, test_idx, train_lab, test_lab = train_test_split(dataFrame, label, 
                                                            test_size = test_ratio, 
                                                            shuffle=True, stratify=label)
"""
train_idx, val_idx, train_lab, val_lab = train_test_split(train_idx, train_lab,
                                                          test_size = validation_ratio,
                                                          shuffle=True, stratify=train_lab)
"""


print("Train size is {}, test size is {}".format(len(train_idx), len(test_idx)))

print("****************Dataset proportion after stratified splitting***************")
print("Train data class proportions:", get_class_proportion(train_idx))
print("Test data class proportions:", get_class_proportion(test_idx))

#parameters
batch_size = 5
num_workers = 1
num_classes = 2       #landfill or background
epochs = 40
lr = 1e-3
w_decay = 1e-5
momentum = 0.9
step_size = 10
gamma = 0.5
image_size = 512
patch_width = 512
patch_height= 512

"""Create custom transforms to perform data augmentation in the class"""

class CustomLandfillDataset(torch.utils.data.Dataset):
  def __init__(self, data, dsType, labelCSV, imgpath, jsonpath, transforms=None, num_classes=num_classes):       #transforms
    self.data = data
    self.labelCSV = labelCSV
    self.json_frame = pd.read_csv(self.labelCSV, usecols=["json index"])
    self.isLandfillList = pd.read_csv(self.labelCSV, usecols=["IsLandfill"]).values.tolist()
    self.num_classes = num_classes
    self.dsType = dsType
    self.imgpath = imgpath
    self.jsonpath = jsonpath

  def transform_train(self, image, mask):
    #convert image and mask to PIL Images
    mask = TF.to_pil_image(mask)
    image = Image.fromarray(image, "RGB")
    #npimg = np.asarray(image)    

    #random horizontal flip
    if random.random() > 0.5:
      image = TF.hflip(image)
      mask = TF.hflip(mask)

    #random vertical flip
    if random.random() > 0.5:
      image = TF.vflip(image)
      mask = TF.vflip(mask)

    #random rotation
    image = TF.rotate(image, angle=10.0)
    mask = TF.rotate(mask, angle=10.0)

    #Gaussian Blur
    image = TF.gaussian_blur(image, kernel_size=(3,3))
    
    #convert PIL image and mask to tensor before returning
    image = TF.to_tensor(image)
    mask = TF.to_tensor(mask)
    
    #normalise the image as per mean and std dev
    #for ImageNet pre-trained
    image = TF.normalize(image, mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])
    #for solaris pre-trained
    #image = TF.normalize(image, mean=[0.006479, 0.009328, 0.01123],
    #                            std=[0.004986, 0.004964, 0.004950])

    return image, mask

  def transform_val(self, image, mask):
    image = TF.to_tensor(image)
    mask = TF.to_tensor(mask)

    #normalise
    image = TF.normalize(image, mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])

    return image, mask

  def __len__(self):
    return len(self.data)

  def __getitem__(self, index):
    #image id
    img_id = self.data[index][0]
    #path for image
    img_name = self.data[index][1]
    #read the image to extract number of channels
    raster_img = rasterio.open(os.path.join(self.imgpath, img_name)).read()
    #change datatype of image from uint16 to int32
    raster_img = raster_img.astype('uint8')
    #print(raster_img.shape)
    raster_channels = raster_img.shape[0]
    #mask
    json_list = self.json_frame.values.tolist()
    json_name = json_list[img_id-1][0]
    IsLandfill = self.isLandfillList[img_id-1][0]
    if(IsLandfill):
      fp_mask = sol.vector.mask.footprint_mask(df=os.path.join(self.jsonpath, json_name), 
                                               reference_im=os.path.join(self.imgpath, img_name))
      fp_mask = fp_mask.astype('uint8')
    else:
      fp_mask = np.zeros((patch_height, patch_width), dtype=np.uint8)

    rgb_image = raster_img.transpose(1,2,0)         
    if(raster_channels == 8):
      """
      Worldview-3 spectral bands
      0 - coastal blue, 1 - blue, 2 - green, 3 - yellow
      4 - red, 5 - red edge, 6 - NIR1, 7 - NIR 2
      """
      rgb_image = np.dstack((raster_img[4,:,:],         
                             raster_img[2,:,:], 
                             raster_img[1,:,:]))
    elif(raster_channels == 4):
      """
      Geoeye-1 sepctral bands
      0 - blue, 1 - green, 2 - red, 3 - NIR
      """
      rgb_image = np.dstack((raster_img[2,:,:], 
                             raster_img[1,:,:], 
                             raster_img[0,:,:]))
      

    min = np.min(rgb_image)
    max = np.max(rgb_image)
    #before normalisation, the image should be brought into a 0 - 1 range (standardisation)
    #rgb_image = ((rgb_image - min) / (max - min))     #.astype('uint8')
    rgb_image = rgb_image.astype('uint8')
    #print("raster image-before:",rgb_image)
    
    #print(image.shape, fp_mask.shape)
    if(self.dsType == 'train'):
      rgb_image, fp_mask = self.transform_train(rgb_image, fp_mask)
    if(self.dsType == 'val'):
      rgb_image, fp_mask = self.transform_val(rgb_image, fp_mask)

    rgb_image_resized = rgb_image.detach().numpy()
    #print("raster image-after:",rgb_image)

    #in case the image is smaller than 512x512
    if((rgb_image.shape[1] < patch_height) or (rgb_image.shape[2] < patch_width)):
      rgb_image_resized = np.zeros((patch_height, patch_width), dtype=np.uint8)
      rgb_image_resized[0:rgb_image.shape[1], 0:rgb_image.shape[2]] = rgb_image[0,:,:]
      rgb_image_resized = np.expand_dims(rgb_image_resized, axis=0)
      for c in range(rgb_image.shape[0]-1):
        resized_raster = np.zeros((patch_height, patch_width), dtype=np.uint8)
        resized_raster[0:rgb_image.shape[1], 0:rgb_image.shape[2]] = rgb_image[c+1,:,:]
        resized_raster = np.expand_dims(resized_raster,axis=0)
        rgb_image_resized = np.vstack((rgb_image_resized, resized_raster)) 

    #rgb_image_resized = rgb_image_resized.astype('uint8')
    #create another mask variable

    mask = fp_mask
    mask = mask.detach().numpy().squeeze()

    #in case the mask size is smaller than 512x512
    if((fp_mask.shape[1] < patch_height) or (fp_mask.shape[2] < patch_width)):
      mask = np.zeros((patch_height, patch_width), dtype=np.int8)
      mask[0:fp_mask.shape[1], 0:fp_mask.shape[2]] = fp_mask
    #one hot encoding of the mask depending on the number of classes
    mask_hotEnc = torch.zeros(self.num_classes, patch_height, patch_width)
    for n in range(self.num_classes):
      mask_hotEnc[n][mask==n] = 1

    #print("resized image:",rgb_image_resized)
    rgb_image_resized = rgb_image_resized.astype('float32')
    mask = mask.astype('float32')
    mask_hotEnc = mask_hotEnc.detach().numpy().astype('float32')

    img_info = {}
    img_info["RGBimage"] = rgb_image_resized
    img_info["mask"] = mask
    img_info["maskHotEnc"] = mask_hotEnc
    img_info["channels"] = raster_channels
    img_info["name"] = img_name
    img_info["image_id"] = img_id

    return img_info

"""Create respective train and test datasets and create a dataloader"""

#train_idx and test_idx are dataframes
train_dataset = CustomLandfillDataset(data=train_idx.values.tolist(), dsType = 'train', transforms=None,
                                      labelCSV=train_labels, imgpath=train_path, jsonpath=pansharpened_json)                               
test_dataset = CustomLandfillDataset(data=test_idx.values.tolist(), dsType = 'train', transforms=None,
                                     labelCSV=train_labels, imgpath=train_path, jsonpath=pansharpened_json)                                

#data loader
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                           batch_size=batch_size,
                                           num_workers=num_workers,
                                           shuffle=True)

test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                           batch_size=batch_size,
                                           num_workers=num_workers,
                                           shuffle=True)

print("length of training dataset:", len(train_dataset))
print("length of test dataset:", len(test_dataset))
print("length of training loader:", len(train_loader))
print("length of test loader:", len(test_loader))

"""Checking out some images and their corresponding masks"""

img_info = next(iter(train_loader))

rgb_img = img_info['RGBimage']
mask = img_info['mask']
mask_hotEnc = img_info['maskHotEnc']
channels = img_info['channels']

print("image mask:", mask.shape, "channels:", channels)
print("transformed image shape:", rgb_img.shape, "hot encoded mask:", mask_hotEnc.shape)
print("image dtype:", rgb_img.dtype, " mask dtype:", mask.dtype, " hot encoded dtype:", mask_hotEnc.dtype)
#check the min and max values of the image and mask to check if it is standardised and normalised
rgbArr = rgb_img[0].detach().numpy()
maskArr = mask_hotEnc[0].detach().numpy()
print("Min value of image:", np.min(rgbArr), " Max value of image:", np.max(rgbArr))
print("Min value of mask:", np.min(maskArr), " Max value of mask:", np.max(maskArr))

test_info = next(iter(test_loader))

rgb_img = img_info['RGBimage']
mask = img_info['mask']
mask_hotEnc = img_info['maskHotEnc']
channels = img_info['channels']

print("image mask:", mask.shape, "channels:", channels)
print("transformed image shape:", rgb_img.shape, "hot encoded mask:", mask_hotEnc.shape)
#check the min and max values of the image and mask to check if it is standardised and normalised
rgbArr = rgb_img[0].detach().numpy()
maskArr = mask_hotEnc[0].detach().numpy()
print("Min value of image:", np.min(rgbArr), " Max value of image:", np.max(rgbArr))
print("Min value of mask:", np.min(maskArr), " Max value of mask:", np.max(maskArr))

"""Draw polygon on image:

"""

count = 0
fig, axs = plt.subplots(batch_size, 5, figsize = (15, 15))

json_frame = pd.read_csv(train_labels, usecols=["json index"])
json_list = json_frame.values.tolist()

for idx in np.arange(batch_size):
  id = img_info["image_id"][idx].item()
  #print(id)
  json_name = json_list[id-1][0]
  name = img_info["name"][idx]
  #print(name)
  channels = img_info["channels"][idx].item()  
  #image = im_convert(name, channels)
  image = img_info["RGBimage"][idx].detach().numpy()
  image = image.transpose(1,2,0)
  image = image.astype('uint8')
  mask = img_info["mask"][idx].detach().numpy()
  #print(mask)
  mask_idx = (img_info["mask"][idx] == 1).sum()
  print(mask_idx)

  maskHotEnc = img_info["maskHotEnc"][idx].detach().numpy()
  #print(maskHotEnc.shape)
  maskHotEnc_split = np.vsplit(maskHotEnc, 2)
  #print(maskHotEnc_split[0].shape)
  axs[count][0].title.set_text(name)
  axs[count][0].imshow(image)

  axs[count][1].title.set_text('Mask')
  axs[count][1].imshow(mask, cmap='gray')

  axs[count][2].title.set_text(name+" with mask")
  axs[count][2].imshow(image)
  axs[count][2].imshow(mask, cmap='ocean', alpha=0.4)

  axs[count][3].title.set_text('Mask Split 1')
  axs[count][3].imshow(maskHotEnc_split[0].transpose(1,2,0).squeeze(), cmap='gray')

  axs[count][4].title.set_text('Mask Split 2')
  axs[count][4].imshow(maskHotEnc_split[1].transpose(1,2,0).squeeze(), cmap='gray')


  count += 1

fig.tight_layout()

"""Plot test loader images and masks"""

count = 0
fig, axs = plt.subplots(batch_size, 5, figsize = (15, 15))


for idx in np.arange(batch_size):
  id = test_info["image_id"][idx].item()
  #print(id)
  json_name = json_list[id-1][0]
  name = test_info["name"][idx]
  #print(name)
  channels = test_info["channels"][idx].item()  
  #image = im_convert(name, channels)
  image = test_info["RGBimage"][idx].detach().numpy()
  image = image.transpose(1,2,0)
  image = image.astype('uint8')
  mask = test_info["mask"][idx].detach().numpy()
  maskHotEnc = test_info["maskHotEnc"][idx].detach().numpy()
  #print(maskHotEnc.shape)
  maskHotEnc_split = np.vsplit(maskHotEnc, 2)
  #print(maskHotEnc_split[0].shape)
  axs[count][0].title.set_text(name)
  axs[count][0].imshow(image)

  axs[count][1].title.set_text('Mask')
  axs[count][1].imshow(mask, cmap='gray')

  axs[count][2].title.set_text(name+" with mask")
  axs[count][2].imshow(image)
  axs[count][2].imshow(mask, cmap='ocean', alpha=0.4)

  axs[count][3].title.set_text('Mask Split 1')
  axs[count][3].imshow(maskHotEnc_split[0].transpose(1,2,0).squeeze(), cmap='gray')

  axs[count][4].title.set_text('Mask Split 2')
  axs[count][4].imshow(maskHotEnc_split[1].transpose(1,2,0).squeeze(), cmap='gray')


  count += 1

fig.tight_layout()

"""Training a model on the training batches

Freeze the weights until some intermediate layers to finetune the rest of the network\
https://discuss.pytorch.org/t/how-the-pytorch-freeze-network-in-some-layers-only-the-rest-of-the-training/7088/2  \
ref: https://github.com/pochih/FCN-pytorch/tree/8436fab3586f118eb36265dab4c5f900748bb02d

FCN encoder
"""

pretrained = False
remove_fc = False
pretrained_model_path ='./PreTrainedModels/xdxd_spacenet4_solaris_weights.pth'
enc_model = 'SolarisPreTrained'

maxpool_ranges = {
    'vgg11'             : ((0, 3), (3, 6),  (6, 11),  (11, 16), (16, 21)),
    'vgg13'             : ((0, 5), (5, 10), (10, 15), (15, 20), (20, 25)),
    'vgg16'             : ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31)),
    'SolarisPreTrained' : ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31)),
    'vgg19'             : ((0, 5), (5, 10), (10, 19), (19, 28), (28, 37))
}

def load_model(enc_model=enc_model):
  if enc_model == 'vgg11':
    model = models.vgg11(pretrained=pretrained)
  if enc_model == 'vgg13':
    model = models.vgg13(pretrained=pretrained)
  if enc_model == 'vgg16':
    print('---loading vgg16----')
    model = models.vgg16(pretrained=pretrained)
  if enc_model == 'vgg19':
    model = models.vgg19(pretrained=pretrained)
  if enc_model == 'SolarisPreTrained':
    print('---loading solaris pretrained---')   
    model = models.vgg16(pretrained=pretrained)
    del model.avgpool
    del model.classifier
    pretrained_dict = torch.load(pretrained_model_path, map_location='cpu')
    model_dict = model.state_dict()
    new = list(pretrained_dict.items())
    count = 0
    for key, value in model_dict.items():
      layer_name, weights = new[count]
      model_dict[key] = weights
      count += 1
    model.load_state_dict(model_dict, strict=False)
  return model

"""FCN model with VGG encoder"""

class FCN8s(nn.Module):
  def __init__(self, pretrained=pretrained, enc_model=enc_model, requires_grad=True, remove_fc=remove_fc, num_classes=num_classes):
    super().__init__()
    self.num_classes = num_classes
    self.enc_model   = load_model(enc_model)
    self.relu    = nn.ReLU(inplace=True)
    self.maxpool_ranges = maxpool_ranges[enc_model]
        #use this piece of code to freeze and unfreeze layers for training

    """
    cnt = 0
    for child in self.enc_model.children():
        for name, param in child.named_parameters():
          if cnt < 4:
            #print(name)
            param.requires_grad = False
          cnt += 1
    """

    if requires_grad == False:
      for param in self.enc_model.parameters():
        param.requires_grad = False
    if remove_fc:
      del self.enc_model.avgpool
      del self.enc_model.classifier
      #print(self.enc_model)
    self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)
    self.bn1     = nn.BatchNorm2d(512)
    self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)
    self.bn2     = nn.BatchNorm2d(256)
    self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)
    self.bn3     = nn.BatchNorm2d(128)
    self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)
    self.bn4     = nn.BatchNorm2d(64)
    self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)
    self.bn5     = nn.BatchNorm2d(32)
    self.classifier = nn.Conv2d(32, num_classes, kernel_size=1)
    #output = self.pretrained_model.forward()
    #print(output)

  def EncoderOutput(self, x):
    output = {}
    for idx in range(len(self.maxpool_ranges)):
      for layer in range(self.maxpool_ranges[idx][0], self.maxpool_ranges[idx][1]):
         x = self.enc_model.features[layer](x)
      output["x%d"%(idx+1)] = x
    return output

  def forward(self, x):
    output = self.EncoderOutput(x)
    #print(output)
    x5 = output['x5']                                 # size=(N, 512, x.H/32, x.W/32)
    #print(x5)
    x4 = output['x4']                                 # size=(N, 512, x.H/16, x.W/16)
    #print(x4)
    x3 = output['x3']                                 # size=(N, 256, x.H/8,  x.W/8)
    #print(x3)

    score = self.relu(self.deconv1(x5))               # size=(N, 512, x.H/16, x.W/16)
    #print(score)
    score = self.bn1(score + x4)                      # element-wise add, size=(N, 512, x.H/16, x.W/16)
    score = self.relu(self.deconv2(score))            # size=(N, 256, x.H/8, x.W/8)
    score = self.bn2(score + x3)                      # element-wise add, size=(N, 256, x.H/8, x.W/8)
    score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)
    score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)
    score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)
    score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)

    return score  # size=(N, n_class, x.H/1, x.W/1)

FCN_model = FCN8s(pretrained=pretrained, enc_model=enc_model, remove_fc=remove_fc,num_classes=num_classes)
log_file = open('./log.txt', 'a')
for name, param in FCN_model.named_parameters():
  if param.requires_grad == True:
    print(name, "\t", param.size())
    log_file.write('{}: {}\n'.format(name, param.size()))

log_file.write(f'**************************************************************\n')
log_file.close()
FCN_model = FCN_model.to(device)

"""Loss and optimiser"""

criterion = nn.BCEWithLogitsLoss() #BCEWithLogits()
optimizer = optim.Adam(FCN_model.parameters(), lr=lr, weight_decay=w_decay) 
scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)

def modelPath(enc_model=enc_model):
  if(enc_model == 'vgg11'):
    model_path = './vgg_11_FCN.pth'
  if(enc_model == 'vgg13'):
    model_path = './vgg_13_FCN.pth'
  if(enc_model == 'vgg16'):
    model_path = './vgg_16_FCN.pth'
  if(enc_model == 'vgg19'):
    model_path = './vgg_19_FCN.pth'
  if(enc_model == 'SolarisPreTrained'):
    model_path = './Solaris_vgg16.pth'

  return model_path

"""K-fold prep"""

#https://github.com/pochih/FCN-pytorch/blob/8436fab3586f118eb36265dab4c5f900748bb02d/python/train.py
model_path = modelPath(enc_model)
score_dir = './'

meanPrecision = []
meanRecall = []
meanSpecificity = []
meanIoU = []
val_losses_min = []
ious_mean = []
train_iou_mean = []
train_precision_mean = []
train_recall_mean = []
train_specificity_mean = []
val_precision_mean = []
val_recall_mean = []
val_specificity_mean = []
val_kappa_mean = []
ious_ = []
pixel_accs = []
train_losses = []
val_losses = []
accuracy = []
pixelAccs_mean = []

def TrainFunction(callbacks):
  val_track = float('inf')

  for epoch in range(epochs):
    train_loss = 0.0
    training_accuracy = 0.0
    total_train = 0.0
    correct_train = 0.0
    total_val = 0.0
    correct_val = 0.0
    val_accuracy = 0.0
    val_loss = 0.0
    trainIoU = 0.0
    iouVal = 0.0
    precisionVal = 0.0
    recallVal = 0.0
    specificityVal = 0.0
    kappaVal = 0.0
    train_ious = []
    total_ious = []
    val_precision = []
    val_recall = []
    val_specificity = []
    val_kappa = []

    t0 = time.time()
    FCN_model.train()
    for batch, img_info in enumerate(train_loader):
      rgb_img = img_info['RGBimage']
      #print("rgb_img:",rgb_img.shape)
      #print(rgb_img)
      mask_hotEnc = img_info['maskHotEnc']
      target = img_info['mask']
      rgb_img = rgb_img.to(device)
      mask_hotEnc = mask_hotEnc.to(device)
      target = target.to(device)
      outputs = FCN_model(rgb_img)
      #print("outputs:",outputs)
      loss = criterion(outputs, mask_hotEnc)
      train_loss += loss.item()*rgb_img.size(0)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

      #training performance metrics
      _, predicted = torch.max(outputs, 1)  
      #print("predicted:",predicted)
      total_train += target.nelement()
      for p, t in zip(predicted, target):
        correct_train += (p == t).sum()
        #train_ious.append(iou(p, t))
        _, _, _, _, iou = ConfusionMatrixComponents(p, t)
        train_ious.append(iou)

    #print("total_train:", total_train)
    train_ious = np.array(train_ious).T
    trainIoU = np.nanmean(train_ious)
    train_iou_mean.append(trainIoU)
    training_accuracy = 100*(correct_train / total_train)
    train_loss = train_loss/len(train_loader.sampler)
    train_losses.append(train_loss)
    accuracy.append(training_accuracy.item())
      
    FCN_model.eval()
    with torch.no_grad():
      for batch, img_info in enumerate(test_loader):
        rgb_img = img_info['RGBimage']
        mask_hotEnc = img_info['maskHotEnc']
        target = img_info['mask']
        rgb_img = rgb_img.to(device)
        mask_hotEnc = mask_hotEnc.to(device)
        target = target.to(device)
        outputs = FCN_model(rgb_img)
        #print("outputs:",outputs)
        loss = criterion(outputs, mask_hotEnc)
        val_loss += loss.item()*rgb_img.size(0)
          
        _, predicted = torch.max(outputs, 1)
        #print("predicted:",predicted)
        total_val += target.nelement()
        #determine performance metrics
        #a. Overall Accuracy
        for p, t in zip(predicted, target):
          correct_val += (p == t).sum()
          #ious are calculated for each class separately and then averaged over all classes
          #to provide global, mean IoU score of our semantic segmentation prediction
          #total_ious.append(iou(p, t))
          precisionVal, recallVal, specificityVal, kappaVal, iou = ConfusionMatrixComponents(p, t)
          val_precision.append(precisionVal)
          val_recall.append(recallVal)
          val_specificity.append(specificityVal)
          #val_kappa.append(kappaVal)
          total_ious.append(iou)

      total_ious = np.array(total_ious).T
      #print(total_ious)
      iouVal = np.nanmean(total_ious)
      ious_mean.append(iouVal)
      val_precision_mean.append(np.mean(val_precision))
      val_recall_mean.append(np.mean(val_recall))
      val_specificity_mean.append(np.mean(val_specificity))
      #val_kappa_mean.append(np.mean(val_kappa))
      val_accuracy = 100 * (correct_val / total_val)
      val_loss = val_loss/len(test_loader.sampler)
      pixelAccs_mean.append(val_accuracy)
      val_losses.append(val_loss)
      
      log_file = open('./log.txt', 'a')
      log_file.write(f'Epoch: [{epoch}/{epochs}] \t Train_loss: {train_loss} \t Val_loss: {val_loss} \t Train_accuracy: {training_accuracy} \t Val_accuracy: {val_accuracy}\n')
      log_file.write(f'Training_IoU: {trainIoU} \t Val_IoU: {iouVal} \t Val_Precision: {np.mean(val_precision)*100} \t Val_Recall: {np.mean(val_recall)*100} \t Val_Specificity: {np.mean(val_specificity)*100}\n')
      log_file.write(f'\n')
      log_file.close()
      print("epoch: {}/{}, Training Loss in epoch:{}, validation loss: {}, training accuracy:{}, validation accuracy: {}, time elapsed: {}".format(epoch, 
                                                                                                    epochs, train_loss, val_loss, training_accuracy, val_accuracy,
                                                                                                    time.time() - t0))
      print("Background and landfill IoUs for epoch {}/{}, Training IoU:{}, Validation IoU:{}".format(epoch, epochs, trainIoU, iouVal))
      print("Validation Precision:{}%, Validation Recall:{}%, Validation Specificity:{}%".format(np.mean(val_precision)*100, 
                                                                                                 np.mean(val_recall)*100, 
                                                                                                 np.mean(val_specificity)*100))
      if(val_loss < val_track):
        val_track = val_loss
        print("Saving model----->")
        torch.save(FCN_model.state_dict(), model_path)

    scheduler.step()

  log_file = open('./log.txt', 'a')
  log_file.write(f'Mean_val_precision: {np.nanmean(val_precision_mean)*100} \t Mean_val_recall: {np.nanmean(val_recall_mean)*100} \t Mean_val_specificity: {np.nanmean(val_specificity_mean)*100}\n')
  log_file.write(f'Mean_train_IoU: {np.nanmean(train_iou_mean)*100} \t Mean_val_IoU: {np.nanmean(ious_mean)*100}\n')
  log_file.close()
  print("Mean Validation Precision:{}%, Mean Validation Recall:{}%, Mean Validation Specificity:{}".format(np.nanmean(val_precision_mean)*100, 
                                                                                                            np.nanmean(val_recall_mean)*100, 
                                                                                                            np.nanmean(val_specificity_mean)*100))
  #Final mean IOU, 
  print("Mean Training IOU:{}%, Mean Validation IOU:{}%:".format(np.nanmean(train_iou_mean)*100, np.nanmean(ious_mean)*100))

"""Common semantic segmentation performance metrics"""

def iou(pred, target):
  ious = []
  for cls in range(num_classes):
    pred_inds = (pred == cls)
    target_inds = (target == cls)
    intersection = pred_inds[target_inds].sum()
    union = pred_inds.sum() + target_inds.sum() - intersection
    if(union == 0):
      ious.append(float('nan'))
    else:
      iou = float(intersection) / max(union, 1)
      ious.append(iou.detach().cpu().numpy())
  #print(ious)
  return ious

def ConfusionMatrixComponents(pred, target):
  tp = 0
  tn = 0
  fp = 0
  fn = 0
  precision = 0.0
  recall = 0.0
  specificity = 0.0
  kappa = 0.0
  IoU = 0.0
  #1 is positive class, 0 is negative class

  pred_tp_idx = (pred == 1)
  target_tp_idx = (target == 1)
  tp = pred_tp_idx[target_tp_idx].sum()

  pred_tn_idx = (pred == 0)
  target_tn_idx = (target == 0)
  tn = pred_tn_idx[target_tn_idx].sum()

  pred_fp_idx = (pred == 1)
  target_fp_idx = (target == 0)
  fp = pred_fp_idx[target_fp_idx].sum()

  pred_fn_idx = (pred == 0)
  target_fn_idx = (target == 1)
  fn = pred_fn_idx[target_fn_idx].sum()

  #print("tp:{}, tn:{}, fp:{}, fn:{}".format(tp, tn, fp, fn))

  #precision 
  precision = tp / ( tp + fp )
  precision = precision.detach().cpu().numpy()
  #recall/sensitivity
  recall = tp / (tp + fn)
  recall = recall.detach().cpu().numpy()
  #specificity
  specificity = tn / (tn + fp)
  specificity = specificity.detach().cpu().numpy()
  #IoU
  IoU = tp / (tp + fn + fp)
  IoU = IoU.detach().cpu().numpy()

  total_obs = (tp+fp+tn+fn)
  observed_agg = (tp+tn)/total_obs
  correct_obs = ((tp+fp)*(tp+fn))/total_obs
  incorrect_obs = ((tn+fn)*(tn+fp))/total_obs
  expected_agg = correct_obs + incorrect_obs
  kappa = (observed_agg - expected_agg)/(1-expected_agg)
  kappa = kappa.detach().cpu().numpy()
  #print(precision, recall, specificity, kappa)
  return precision, recall, specificity, kappa, IoU

early_stop_callback = EarlyStopping(
    monitor = 'val_loss',
    min_delta = 0.0,
    patience = 3,
    verbose = False,
    mode = 'min'
)

TrainFunction(callbacks=[early_stop_callback])

def PlotLoss(train_loss, val_loss):
  print('****Loss Plot********')
  plt.plot(train_loss, label='Training loss')
  plt.plot(val_losses, label = 'Validation loss')
  plt.title('Loss plot')
  plt.legend()

def PlotAccuracy(train_accuracy, val_accuracy):
  plt.plot(accuracy, label='Training pixel accuracy')
  plt.plot(pixelAccs_mean, label='Validation pixel accuracy')
  plt.title('Accuracy plot')
  plt.legend()

def PlotIoU(train_iou_mean, ious_mean):
  plt.plot(train_iou_mean, label='Training IoU')
  plt.plot(ious_mean, label='Validation IoU')
  plt.title('IoU plot')
  plt.legend()

def ROC(specificity, recall):
  x = np.linspace(0,1,50)
  y = x
  plt.plot(x,y,'k-')
  plt.plot(specificity, recall)
  plt.xlabel('Specificity')
  plt.ylabel('Recall')
  plt.title('ROC')
  plt.legend()

def PrecisionRecall(precision, recall):
  plt.plot(recall, precision)
  plt.xlabel('Recall')
  plt.ylabel('Precision')
  plt.title('Precision-Recall')
  plt.legend()

PlotLoss(train_losses, val_losses)

PlotAccuracy(accuracy, pixelAccs_mean)

PlotIoU(train_iou_mean, ious_mean)

"""We plot ROC over precision-recall becauase we have more background pixels as compared to the landfill pixels. Precision is more focussed on positive class than on the negative class and it actually measures the probablity of correct detection of positive values. ROC curve on the other hand meaures the ability to distinguish between classes and hence is a better measure for imbalanced classes"""

#extract multispectral images
ExtractFiles(TIF_file_path , os.path.join(out_path, 'MultiSpectralTIF'))
#extract multispectral json files
ExtractFiles(JSON_file_path , os.path.join(out_path, 'LandfillCoordPolygonMulti'))

val_model = FCN8s(pretrained=pretrained, enc_model=enc_model, remove_fc=remove_fc, num_classes=num_classes)
val_model.load_state_dict(torch.load(model_path, map_location='cpu'))
val_model.eval()

valFrame = pd.read_csv(multispectral_labels, usecols=["Idx", "Image Index", "IsLandfill"])
data = valFrame.values.tolist()
label = pd.read_csv(multispectral_labels, usecols=["IsLandfill"])

val_dataset = CustomLandfillDataset(data=data, dsType = 'val', transforms=None, labelCSV=multispectral_labels, 
                                    imgpath=multispectral_path, jsonpath=multispectral_json)                                

#data loader
val_loader = torch.utils.data.DataLoader(dataset=val_dataset,
                                           batch_size=batch_size,
                                           num_workers=num_workers,
                                           shuffle=True)

img_info = next(iter(val_loader))

rgb_img = img_info['RGBimage']
mask = img_info['mask']
mask_hotEnc = img_info['maskHotEnc']
channels = img_info['channels']


rgb_img = rgb_img.to('cpu')
output = val_model(rgb_img)
print("output shape:",output.shape)
_, predicted = torch.max(output, 1)
print("predicted shape:", predicted.shape)

print("********Predicted segments****************")
count = 0
fig, axs = plt.subplots(batch_size, 5, figsize = (15, 15))
for idx in np.arange(batch_size):
  id = img_info["image_id"][idx].item()
  #print(id)
  json_name = json_list[id-1][0]
  name = img_info["name"][idx]
  #print(name)
  channels = img_info["channels"][idx].item()  
  #image = im_convert(name, channels)
  image = img_info["RGBimage"][idx].detach().numpy()
  image = image.transpose(1,2,0)
  #print(image.shape)
  #print(image)
  image = image.astype('uint8')
  mask = img_info["mask"][idx].detach().numpy()
  #print(mask.shape, predicted.shape)
  axs[count][0].title.set_text(name)
  axs[count][0].imshow(image)

  axs[count][1].title.set_text('Mask')
  axs[count][1].imshow(mask, cmap='gray')

  axs[count][2].title.set_text(name+" with mask")
  axs[count][2].imshow(image)
  axs[count][2].imshow(mask, cmap='ocean', alpha=0.4)

  axs[count][3].title.set_text('Predicted mask')
  axs[count][3].imshow(predicted[idx], cmap='gray')

  axs[count][4].title.set_text(name+" with predicted mask")
  axs[count][4].imshow(image)
  axs[count][4].imshow(predicted[idx], cmap='ocean', alpha=0.4)
  
  count += 1

fig.tight_layout()

from google.colab import files
import glob

files.download(model_path)
files.download('./log.txt')